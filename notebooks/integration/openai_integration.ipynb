{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ó Welcome to AdalFlow OpenAI Integration!\n",
    "## Complete Guide to OpenAI's Response API and Model Capabilities\n",
    "\n",
    "Thanks for trying us out! This notebook demonstrates how to use OpenAI's new Response API (responses.create) with various model capabilities including text generation, vision, reasoning, and image generation through tools. üòä\n",
    "\n",
    "Any questions or concerns you may have, [come talk to us on discord,](https://discord.gg/ezzszrRZvT) we're always here to help! ‚≠ê <i>Star us on <a href=\"https://github.com/SylphAI-Inc/AdalFlow\">Github</a> </i> ‚≠ê\n",
    "\n",
    "\n",
    "# Quick Links\n",
    "\n",
    "- Github repo: https://github.com/SylphAI-Inc/AdalFlow\n",
    "- Full Tutorials: https://adalflow.sylph.ai/index.html\n",
    "- OpenAI Models Tutorial: [openai_models.py](https://github.com/SylphAI-Inc/AdalFlow/blob/main/tutorials/models/openai_models.py)\n",
    "- OpenAI Client Source: [openai_client.py](https://github.com/SylphAI-Inc/AdalFlow/blob/main/adalflow/adalflow/components/model_client/openai_client.py)\n",
    "\n",
    "# Author\n",
    "\n",
    "This notebook was created by the AdalFlow Team.\n",
    "\n",
    "# Outline\n",
    "\n",
    "We will cover:\n",
    "1. **OpenAI's Response API** - The new unified API for all model interactions\n",
    "2. **Text Generation** - Basic and streaming responses with GPT models\n",
    "3. **Vision Models** - Multimodal inputs with images (URLs, local files, base64)\n",
    "4. **Reasoning Models** - O1 and O1-mini for complex problem solving\n",
    "5. **Image Generation** - Using the new tools API for DALL-E integration\n",
    "6. **Embeddings** - Text embeddings for similarity search\n",
    "\n",
    "# Installation\n",
    "\n",
    "1. Use `pip` to install the `adalflow` Python package with OpenAI support:\n",
    "\n",
    "  ```bash\n",
    "  pip install adalflow[openai]\n",
    "  ```\n",
    "  \n",
    "2. Set up your OpenAI API key from [platform.openai.com](https://platform.openai.com/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install adalflow with OpenAI support\n",
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install -U adalflow[openai]\n",
    "\n",
    "clear_output()\n",
    "print(\"‚úÖ AdalFlow with OpenAI support installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Prompt user to enter their API key securely\n",
    "openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
    "\n",
    "# Set environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "print(\"‚úÖ API key has been set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Understanding OpenAI's Response API\n",
    "\n",
    "OpenAI has introduced a new **Response API** (`responses.create`) that unifies all model interactions. This replaces the older Chat Completions API and provides:\n",
    "\n",
    "- **Unified Interface**: Single API for text, vision, reasoning, and tool use\n",
    "- **Streaming Support**: Real-time response streaming with typed events\n",
    "- **Tool Integration**: Native support for image generation and other tools\n",
    "- **Structured Output**: Better parsing with typed response objects\n",
    "\n",
    "## Key Differences from Chat Completions API:\n",
    "\n",
    "| Feature | Chat Completions API | Response API |\n",
    "|---------|---------------------|---------------|\n",
    "| Endpoint | `chat.completions.create` | `responses.create` |\n",
    "| Input Format | `messages` | `input` (string or messages) |\n",
    "| Multimodal | Complex content arrays | Simplified with `input` + `images` |\n",
    "| Streaming | Untyped chunks | Typed events (ResponseTextDeltaEvent, etc.) |\n",
    "| Tools | Function calling | Native tool types (e.g., `image_generation`) |\n",
    "\n",
    "Let's see how AdalFlow seamlessly handles this for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Basic Text Generation\n",
    "\n",
    "Let's start with simple text generation using GPT models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adalflow as adal\n",
    "from adalflow.components.model_client import OpenAIClient\n",
    "\n",
    "# Initialize the OpenAI client - it automatically uses the Response API\n",
    "client = OpenAIClient()\n",
    "\n",
    "# Create a generator with GPT-4o-mini\n",
    "generator = adal.Generator(\n",
    "    model_client=client,\n",
    "    model_kwargs={\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 150\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generate a response\n",
    "prompt_kwargs = {\"input_str\": \"Explain quantum computing in simple terms.\"}\n",
    "response = generator(prompt_kwargs)\n",
    "\n",
    "print(\"Response:\", response.data)\n",
    "print(f\"\\nUsage: {response.usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì° Streaming Responses\n",
    "\n",
    "The Response API provides typed streaming events. AdalFlow includes helper functions to extract text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adalflow.components.model_client.utils import extract_text_from_response_stream\n",
    "\n",
    "# Create a streaming generator\n",
    "streaming_generator = adal.Generator(\n",
    "    model_client=OpenAIClient(),\n",
    "    model_kwargs={\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"stream\": True,  # Enable streaming\n",
    "        \"max_tokens\": 200\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generate with streaming\n",
    "prompt_kwargs = {\"input_str\": \"Tell me a short story about a robot learning to paint.\"}\n",
    "response = streaming_generator(prompt_kwargs)\n",
    "\n",
    "print(\"Streaming output:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# The Response API sends typed events like ResponseTextDeltaEvent\n",
    "# Use the helper function to extract text from these events\n",
    "if hasattr(response.raw_response, '__iter__'):\n",
    "    for event in response.raw_response:\n",
    "        text = extract_text_from_response_stream(event)\n",
    "        if text:\n",
    "            print(text, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üëÅÔ∏è Vision Models (Multimodal)\n",
    "\n",
    "OpenAI's vision models can analyze images from URLs, local files, or base64 data.\n",
    "The Response API simplifies multimodal inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Analyze an image from URL\n",
    "vision_generator = adal.Generator(\n",
    "    model_client=OpenAIClient(),\n",
    "    model_kwargs={\n",
    "        \"model\": \"gpt-4o\",  # Vision-capable model\n",
    "        \"images\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/320px-Cat03.jpg\"\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt_kwargs = {\"input_str\": \"Describe this image in detail. What emotions does it convey?\"}\n",
    "response = vision_generator(prompt_kwargs)\n",
    "\n",
    "print(\"Image Analysis:\")\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Compare multiple images\n",
    "multi_image_generator = adal.Generator(\n",
    "    model_client=OpenAIClient(),\n",
    "    model_kwargs={\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"images\": [\n",
    "            \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/320px-Cat03.jpg\",\n",
    "            \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/American_Eskimo_Dog.jpg/320px-American_Eskimo_Dog.jpg\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt_kwargs = {\"input_str\": \"Compare these two images. What are the main differences?\"}\n",
    "response = multi_image_generator(prompt_kwargs)\n",
    "\n",
    "print(\"Comparison Result:\")\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Using local images\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download a test image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/320px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "local_image_path = \"test_nature.jpg\"\n",
    "\n",
    "urllib.request.urlretrieve(image_url, local_image_path)\n",
    "print(f\"Downloaded test image to {local_image_path}\")\n",
    "\n",
    "# Use local image with vision model\n",
    "local_image_generator = adal.Generator(\n",
    "    model_client=OpenAIClient(),\n",
    "    model_kwargs={\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"images\": local_image_path  # Local file path\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt_kwargs = {\"input_str\": \"What season does this image depict? Describe the atmosphere.\"}\n",
    "response = local_image_generator(prompt_kwargs)\n",
    "\n",
    "print(\"\\nLocal Image Analysis:\")\n",
    "print(response.data)\n",
    "\n",
    "# Cleanup\n",
    "os.remove(local_image_path)\n",
    "print(f\"\\nCleaned up {local_image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Reasoning Models (O1 Series)\n",
    "\n",
    "OpenAI's O1 models provide enhanced reasoning capabilities with a \"thinking\" process.\n",
    "Note: O1 models require special access and may have different pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with O1-mini (if you have access)\n",
    "reasoning_generator = adal.Generator(\n",
    "    model_client=OpenAIClient(),\n",
    "    model_kwargs={\n",
    "        \"model\": \"o1-mini\",  # Or \"o1\" for the full model\n",
    "        \"reasoning\": {\n",
    "            \"effort\": \"medium\",  # low, medium, high\n",
    "            \"summary\": \"auto\"    # detailed, auto, none\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Complex reasoning task\n",
    "prompt_kwargs = {\n",
    "    \"input_str\": \"\"\"A farmer needs to transport a fox, a chicken, and a bag of grain across a river. \n",
    "    The boat can only carry the farmer and one item at a time. If left alone, the fox will eat the chicken, \n",
    "    and the chicken will eat the grain. How can the farmer get everything across safely?\"\"\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = reasoning_generator(prompt_kwargs)\n",
    "    print(\"Solution:\", response.data)\n",
    "    if response.thinking:\n",
    "        print(f\"\\nReasoning process: {response.thinking[:500]}...\")  # Show first 500 chars\n",
    "except Exception as e:\n",
    "    print(f\"Note: O1 models require special access. Error: {e}\")\n",
    "    print(\"You can use GPT-4o for reasoning tasks as well.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® Image Generation with Tools API\n",
    "\n",
    "The new Response API integrates image generation through the tools parameter.\n",
    "This replaces the legacy DALL-E API with a unified interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create output directory for generated images\n",
    "Path(\"generated_images\").mkdir(exist_ok=True)\n",
    "\n",
    "# Image generation using tools API\n",
    "image_generator = adal.Generator(\n",
    "    model_client=OpenAIClient(),\n",
    "    model_kwargs={\n",
    "        \"model\": \"gpt-4o-mini\",  # Any model that supports tools\n",
    "        \"tools\": [{\"type\": \"image_generation\"}]  # Enable image generation tool\n",
    "    }\n",
    ")\n",
    "\n",
    "# Request image generation\n",
    "prompt_kwargs = {\n",
    "    \"input_str\": \"Generate an image of a serene Japanese garden with cherry blossoms, \"\n",
    "                 \"a wooden bridge over a koi pond, in watercolor style\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = image_generator(prompt_kwargs)\n",
    "    \n",
    "    print(\"Response Text:\", response.data)\n",
    "    \n",
    "    if response.images:\n",
    "        print(\"\\n‚úÖ Image generated successfully!\")\n",
    "        \n",
    "        # Save the generated image(s)\n",
    "        saved_paths = response.save_images(\n",
    "            directory=\"generated_images\",\n",
    "            prefix=\"japanese_garden\",\n",
    "            format=\"png\"\n",
    "        )\n",
    "        \n",
    "        if isinstance(saved_paths, list):\n",
    "            for path in saved_paths:\n",
    "                print(f\"  Saved to: {path}\")\n",
    "        else:\n",
    "            print(f\"  Saved to: {saved_paths}\")\n",
    "    else:\n",
    "        print(\"No images were generated.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Image generation error: {e}\")\n",
    "    print(\"Note: Image generation requires appropriate API access and credits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Mixed Generation: Text + Images\n",
    "\n",
    "The Response API can generate both text and images in a single call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed content generator\n",
    "mixed_generator = adal.Generator(\n",
    "    model_client=OpenAIClient(),\n",
    "    model_kwargs={\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"tools\": [{\"type\": \"image_generation\"}]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Request both text and image\n",
    "prompt_kwargs = {\n",
    "    \"input_str\": \"\"\"Write a haiku about the ocean at sunset, \n",
    "                    then generate an image that captures the essence of the haiku.\"\"\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = mixed_generator(prompt_kwargs)\n",
    "    \n",
    "    # Display text content (haiku)\n",
    "    if response.data:\n",
    "        print(\"üìù Haiku:\")\n",
    "        print(response.data)\n",
    "    \n",
    "    # Save generated image\n",
    "    if response.images:\n",
    "        print(\"\\nüé® Image generated!\")\n",
    "        saved_path = response.save_images(\n",
    "            directory=\"generated_images\",\n",
    "            prefix=\"ocean_haiku\",\n",
    "            format=\"jpg\"\n",
    "        )\n",
    "        print(f\"  Saved to: {saved_path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Mixed generation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Text Embeddings\n",
    "\n",
    "Generate embeddings for semantic search and similarity comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adalflow.core import Embedder\n",
    "import numpy as np\n",
    "\n",
    "# Create an embedder\n",
    "embedder = Embedder(\n",
    "    model_client=OpenAIClient(),\n",
    "    model_kwargs={\"model\": \"text-embedding-3-small\"}\n",
    ")\n",
    "\n",
    "# Generate embeddings for multiple texts\n",
    "texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"AI enables computers to learn from data.\",\n",
    "    \"The weather is sunny today.\",\n",
    "    \"Deep learning uses neural networks.\"\n",
    "]\n",
    "\n",
    "embeddings = embedder(input=texts)\n",
    "\n",
    "print(f\"Generated {len(embeddings.data)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings.data[0].embedding)}\")\n",
    "\n",
    "# Calculate cosine similarity between first two texts\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "emb1 = np.array(embeddings.data[0].embedding)\n",
    "emb2 = np.array(embeddings.data[1].embedding)\n",
    "emb3 = np.array(embeddings.data[2].embedding)\n",
    "\n",
    "sim_12 = cosine_similarity(emb1, emb2)\n",
    "sim_13 = cosine_similarity(emb1, emb3)\n",
    "\n",
    "print(f\"\\nSimilarity between text 1 and 2 (related): {sim_12:.3f}\")\n",
    "print(f\"Similarity between text 1 and 3 (unrelated): {sim_13:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# üìä Response Structure and Parsing\n\nUnderstanding the Response API output structure:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate response structure\n",
    "demo_generator = adal.Generator(\n",
    "    model_client=OpenAIClient(),\n",
    "    model_kwargs={\"model\": \"gpt-4o-mini\", \"max_tokens\": 50}\n",
    ")\n",
    "\n",
    "response = demo_generator({\"input_str\": \"Say hello\"})\n",
    "\n",
    "print(\"GeneratorOutput fields:\")\n",
    "print(f\"  - data: {response.data}\")\n",
    "print(f\"  - raw_response: {type(response.raw_response).__name__}\")\n",
    "print(f\"  - usage: {response.usage}\")\n",
    "print(f\"  - error: {response.error}\")\n",
    "print(f\"  - images: {response.images}\")\n",
    "print(f\"  - thinking: {response.thinking}\")\n",
    "\n",
    "# The Response API returns structured objects\n",
    "print(f\"\\nResponse API object type: {type(response.raw_response)}\")\n",
    "if hasattr(response.raw_response, 'output_text'):\n",
    "    print(f\"Output text: {response.raw_response.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# üéØ Best Practices\n\n1. **Image Handling**:\n   - URLs are most efficient (no encoding needed)\n   - Local files are automatically encoded to base64\n   - Use `save_images()` helper for convenient image saving\n\n2. **Streaming**:\n   - Use `extract_text_from_response_stream()` helper for text extraction\n   - Handle events based on their type for custom processing\n\n3. **Error Handling**:\n   - Always wrap API calls in try-except blocks\n   - Check for `response.error` field\n   - Monitor usage with `response.usage`\n\n4. **Performance**:\n   - Use appropriate `max_tokens` to control costs\n   - Enable streaming for better user experience\n   - Cache embeddings when possible",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Resources and Next Steps\n",
    "\n",
    "## Documentation\n",
    "- [AdalFlow Documentation](https://adalflow.sylph.ai/)\n",
    "- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)\n",
    "- [OpenAI Models Tutorial](https://github.com/SylphAI-Inc/AdalFlow/blob/main/tutorials/models/openai_models.py)\n",
    "\n",
    "## Source Code\n",
    "- [OpenAI Client Implementation](https://github.com/SylphAI-Inc/AdalFlow/blob/main/adalflow/adalflow/components/model_client/openai_client.py)\n",
    "- [Response API Utils](https://github.com/SylphAI-Inc/AdalFlow/blob/main/adalflow/adalflow/components/model_client/utils.py)\n",
    "\n",
    "## Community\n",
    "- [Discord](https://discord.gg/ezzszrRZvT) - Get help and share experiences\n",
    "- [GitHub Issues](https://github.com/SylphAI-Inc/AdalFlow/issues) - Report bugs or request features\n",
    "- [GitHub Discussions](https://github.com/SylphAI-Inc/AdalFlow/discussions) - Share ideas and feedback\n",
    "\n",
    "## Next Steps\n",
    "1. Try the [auto-optimization tutorial](https://colab.research.google.com/drive/1n3mHUWekTEYHiBdYBTw43TKlPN41A9za?usp=sharing)\n",
    "2. Explore [use cases](https://adalflow.sylph.ai/use_cases/index.html)\n",
    "3. Build your own LLM applications with AdalFlow!\n",
    "\n",
    "Thank you for using AdalFlow! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}